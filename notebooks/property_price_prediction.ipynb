{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707cfbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Current working directory: c:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\notebooks\n",
      "Notebook run time: 2025-07-03 16:22:03.066316\n"
     ]
    }
   ],
   "source": [
    "# Property Price Prediction Model\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Set display options\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Notebook run time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aeabc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Land dataset loaded successfully: (8000, 9)\n",
      "Building dataset loaded successfully: (8980, 22)\n",
      "\n",
      "==================================================\n",
      "LAND DATASET OVERVIEW\n",
      "==================================================\n",
      "Shape: (8000, 9)\n",
      "\n",
      "Column names:\n",
      "['Kecamatan', 'Lokasi', 'Luas_m2', 'Sertifikat', 'Kondisi', 'Tipe_Iklan', 'NJOP_Rp_per_m2', 'Jumlah_Penduduk', 'Aksesibilitas']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kecamatan</th>\n",
       "      <th>Lokasi</th>\n",
       "      <th>Luas_m2</th>\n",
       "      <th>Sertifikat</th>\n",
       "      <th>Kondisi</th>\n",
       "      <th>Tipe_Iklan</th>\n",
       "      <th>NJOP_Rp_per_m2</th>\n",
       "      <th>Jumlah_Penduduk</th>\n",
       "      <th>Aksesibilitas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gubeng,\"Jl. Tubagus Ismail, Gubeng, Surabaya\",...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sawahan,\"Jl. Siliwangi, Sawahan, Surabaya\",112...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asemrowo,\"Gg. Rajawali Barat, Asemrowo, Suraba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Karang Pilang,\"Jalan Veteran, Karang Pilang, S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sawahan,\"Gg. H.J Maemunah, Sawahan, Surabaya\",...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Kecamatan  Lokasi  Luas_m2  \\\n",
       "0  Gubeng,\"Jl. Tubagus Ismail, Gubeng, Surabaya\",...     NaN      NaN   \n",
       "1  Sawahan,\"Jl. Siliwangi, Sawahan, Surabaya\",112...     NaN      NaN   \n",
       "2  Asemrowo,\"Gg. Rajawali Barat, Asemrowo, Suraba...     NaN      NaN   \n",
       "3  Karang Pilang,\"Jalan Veteran, Karang Pilang, S...     NaN      NaN   \n",
       "4  Sawahan,\"Gg. H.J Maemunah, Sawahan, Surabaya\",...     NaN      NaN   \n",
       "\n",
       "   Sertifikat  Kondisi  Tipe_Iklan  NJOP_Rp_per_m2  Jumlah_Penduduk  \\\n",
       "0         NaN      NaN         NaN             NaN              NaN   \n",
       "1         NaN      NaN         NaN             NaN              NaN   \n",
       "2         NaN      NaN         NaN             NaN              NaN   \n",
       "3         NaN      NaN         NaN             NaN              NaN   \n",
       "4         NaN      NaN         NaN             NaN              NaN   \n",
       "\n",
       "   Aksesibilitas  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading and Initial Exploration\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Define data paths\n",
    "land_data_path = '../data/raw/Dataset_Tanah_Surabaya.csv'\n",
    "building_data_path = '../data/raw/Dataset_Bangunan_Surabaya.csv'\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    df_land = pd.read_csv(land_data_path)\n",
    "    print(f\"Land dataset loaded successfully: {df_land.shape}\")\n",
    "    \n",
    "    df_building = pd.read_csv(building_data_path)\n",
    "    print(f\"Building dataset loaded successfully: {df_building.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    \n",
    "# Display basic information about the datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LAND DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df_land.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_land.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_land.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d5e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BUILDING DATASET OVERVIEW\n",
      "==================================================\n",
      "Shape: (8980, 22)\n",
      "\n",
      "Column names:\n",
      "['Kecamatan', 'Kamar Tidur', 'Kamar Mandi', 'Luas Tanah', 'Luas Bangunan', 'Sertifikat', 'Daya Listrik', 'Ruang Makan', 'Ruang Tamu', 'Kondisi Perabotan', 'Jumlah Lantai', 'Hadap', 'Terjangkau Internet', 'Lebar Jalan', 'Sumber Air', 'Hook', 'Kondisi Properti', 'Alamat', 'Tipe Iklan', 'Aksesibilitas', 'NJOP_Rp_per_m2', 'Tingkat_Keamanan']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kecamatan</th>\n",
       "      <th>Kamar Tidur</th>\n",
       "      <th>Kamar Mandi</th>\n",
       "      <th>Luas Tanah</th>\n",
       "      <th>Luas Bangunan</th>\n",
       "      <th>Sertifikat</th>\n",
       "      <th>Daya Listrik</th>\n",
       "      <th>Ruang Makan</th>\n",
       "      <th>Ruang Tamu</th>\n",
       "      <th>Kondisi Perabotan</th>\n",
       "      <th>Jumlah Lantai</th>\n",
       "      <th>Hadap</th>\n",
       "      <th>Terjangkau Internet</th>\n",
       "      <th>Lebar Jalan</th>\n",
       "      <th>Sumber Air</th>\n",
       "      <th>Hook</th>\n",
       "      <th>Kondisi Properti</th>\n",
       "      <th>Alamat</th>\n",
       "      <th>Tipe Iklan</th>\n",
       "      <th>Aksesibilitas</th>\n",
       "      <th>NJOP_Rp_per_m2</th>\n",
       "      <th>Tingkat_Keamanan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wonokromo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>70</td>\n",
       "      <td>SHM - Sertifikat Hak Milik</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Barat</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Baru</td>\n",
       "      <td>Jl. Wonokromo Raya No.103, Surabaya</td>\n",
       "      <td>Keduanya</td>\n",
       "      <td>Baik</td>\n",
       "      <td>1032000.0</td>\n",
       "      <td>Tinggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rungkut</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2023</td>\n",
       "      <td>SHM - Sertifikat Hak Milik</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Selatan</td>\n",
       "      <td>Ya</td>\n",
       "      <td>2 Mobil</td>\n",
       "      <td>PAM atau PDAM</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Baru</td>\n",
       "      <td>Jl. Rungkut Raya No.180, Surabaya</td>\n",
       "      <td>Keduanya</td>\n",
       "      <td>Baik</td>\n",
       "      <td>1032000.0</td>\n",
       "      <td>Tinggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semampir</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>85</td>\n",
       "      <td>SHM - Sertifikat Hak Milik</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Selatan</td>\n",
       "      <td>Ya</td>\n",
       "      <td>2 Mobil</td>\n",
       "      <td>PAM atau PDAM</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Bagus</td>\n",
       "      <td>Jl. Semampir Raya No.93, Surabaya</td>\n",
       "      <td>Keduanya</td>\n",
       "      <td>Baik</td>\n",
       "      <td>1274000.0</td>\n",
       "      <td>Rendah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pakal</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>91</td>\n",
       "      <td>HGB - Hak Guna Bangunan</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Barat</td>\n",
       "      <td>Ya</td>\n",
       "      <td>2 Mobil</td>\n",
       "      <td>Sumur Pompa</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Baru</td>\n",
       "      <td>Jl. Pakal Raya No.15, Surabaya</td>\n",
       "      <td>Disewa</td>\n",
       "      <td>Buruk</td>\n",
       "      <td>1032000.0</td>\n",
       "      <td>Rendah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gayungan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>SHM - Sertifikat Hak Milik</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PAM atau PDAM</td>\n",
       "      <td>Tidak</td>\n",
       "      <td>Baru</td>\n",
       "      <td>Jl. Gayungan Raya No.107, Surabaya</td>\n",
       "      <td>Disewa</td>\n",
       "      <td>Baik</td>\n",
       "      <td>1032000.0</td>\n",
       "      <td>Tinggi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Kecamatan  Kamar Tidur  Kamar Mandi  Luas Tanah  Luas Bangunan  \\\n",
       "0  Wonokromo            3            2          45             70   \n",
       "1    Rungkut            3            2          48           2023   \n",
       "2   Semampir            3            3          48             85   \n",
       "3      Pakal            2            1          50             91   \n",
       "4   Gayungan            2            2          51             50   \n",
       "\n",
       "                   Sertifikat  Daya Listrik Ruang Makan Ruang Tamu  \\\n",
       "0  SHM - Sertifikat Hak Milik        1300.0       Tidak      Tidak   \n",
       "1  SHM - Sertifikat Hak Milik        1300.0          Ya         Ya   \n",
       "2  SHM - Sertifikat Hak Milik        1300.0          Ya         Ya   \n",
       "3     HGB - Hak Guna Bangunan        1300.0          Ya         Ya   \n",
       "4  SHM - Sertifikat Hak Milik        2200.0       Tidak      Tidak   \n",
       "\n",
       "  Kondisi Perabotan  Jumlah Lantai    Hadap Terjangkau Internet Lebar Jalan  \\\n",
       "0       Unfurnished            2.0    Barat               Tidak         NaN   \n",
       "1       Unfurnished            2.0  Selatan                  Ya     2 Mobil   \n",
       "2       Unfurnished            3.0  Selatan                  Ya     2 Mobil   \n",
       "3       Unfurnished            1.0    Barat                  Ya     2 Mobil   \n",
       "4       Unfurnished            NaN      NaN               Tidak         NaN   \n",
       "\n",
       "      Sumber Air   Hook Kondisi Properti  \\\n",
       "0            NaN  Tidak             Baru   \n",
       "1  PAM atau PDAM  Tidak             Baru   \n",
       "2  PAM atau PDAM  Tidak            Bagus   \n",
       "3    Sumur Pompa     Ya             Baru   \n",
       "4  PAM atau PDAM  Tidak             Baru   \n",
       "\n",
       "                                 Alamat Tipe Iklan Aksesibilitas  \\\n",
       "0  Jl. Wonokromo Raya No.103, Surabaya    Keduanya          Baik   \n",
       "1    Jl. Rungkut Raya No.180, Surabaya    Keduanya          Baik   \n",
       "2    Jl. Semampir Raya No.93, Surabaya    Keduanya          Baik   \n",
       "3       Jl. Pakal Raya No.15, Surabaya      Disewa         Buruk   \n",
       "4   Jl. Gayungan Raya No.107, Surabaya      Disewa          Baik   \n",
       "\n",
       "   NJOP_Rp_per_m2 Tingkat_Keamanan  \n",
       "0       1032000.0           Tinggi  \n",
       "1       1032000.0           Tinggi  \n",
       "2       1274000.0           Rendah  \n",
       "3       1032000.0           Rendah  \n",
       "4       1032000.0           Tinggi  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Dataset Overview\n",
    "print(\"=\"*50)\n",
    "print(\"BUILDING DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df_building.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_building.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_building.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea01cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA QUALITY ASSESSMENT - LAND DATASET\n",
      "==================================================\n",
      "Dataset shape: (8000, 9)\n",
      "\n",
      "1. Data Types:\n",
      "Kecamatan           object\n",
      "Lokasi             float64\n",
      "Luas_m2            float64\n",
      "Sertifikat         float64\n",
      "Kondisi            float64\n",
      "Tipe_Iklan         float64\n",
      "NJOP_Rp_per_m2     float64\n",
      "Jumlah_Penduduk    float64\n",
      "Aksesibilitas      float64\n",
      "dtype: object\n",
      "\n",
      "2. Missing Values:\n",
      "                 Missing Count  Missing Percentage\n",
      "Lokasi                    8000               100.0\n",
      "Luas_m2                   8000               100.0\n",
      "Sertifikat                8000               100.0\n",
      "Kondisi                   8000               100.0\n",
      "Tipe_Iklan                8000               100.0\n",
      "NJOP_Rp_per_m2            8000               100.0\n",
      "Jumlah_Penduduk           8000               100.0\n",
      "Aksesibilitas             8000               100.0\n",
      "\n",
      "3. Basic Statistics:\n",
      "       Lokasi  Luas_m2  Sertifikat  Kondisi  Tipe_Iklan  NJOP_Rp_per_m2  \\\n",
      "count     0.0      0.0         0.0      0.0         0.0             0.0   \n",
      "mean      NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "std       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "min       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "25%       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "50%       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "75%       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "max       NaN      NaN         NaN      NaN         NaN             NaN   \n",
      "\n",
      "       Jumlah_Penduduk  Aksesibilitas  \n",
      "count              0.0            0.0  \n",
      "mean               NaN            NaN  \n",
      "std                NaN            NaN  \n",
      "min                NaN            NaN  \n",
      "25%                NaN            NaN  \n",
      "50%                NaN            NaN  \n",
      "75%                NaN            NaN  \n",
      "max                NaN            NaN  \n",
      "\n",
      "4. Unique Values per Column:\n",
      "Kecamatan: 7999 unique values\n",
      "Lokasi: 0 unique values\n",
      "Luas_m2: 0 unique values\n",
      "Sertifikat: 0 unique values\n",
      "Kondisi: 0 unique values\n",
      "Tipe_Iklan: 0 unique values\n",
      "NJOP_Rp_per_m2: 0 unique values\n",
      "Jumlah_Penduduk: 0 unique values\n",
      "Aksesibilitas: 0 unique values\n",
      "\n",
      "==================================================\n",
      "DATA QUALITY ASSESSMENT - BUILDING DATASET\n",
      "==================================================\n",
      "Dataset shape: (8980, 22)\n",
      "\n",
      "1. Data Types:\n",
      "Kecamatan               object\n",
      "Kamar Tidur              int64\n",
      "Kamar Mandi              int64\n",
      "Luas Tanah               int64\n",
      "Luas Bangunan            int64\n",
      "Sertifikat              object\n",
      "Daya Listrik           float64\n",
      "Ruang Makan             object\n",
      "Ruang Tamu              object\n",
      "Kondisi Perabotan       object\n",
      "Jumlah Lantai          float64\n",
      "Hadap                   object\n",
      "Terjangkau Internet     object\n",
      "Lebar Jalan             object\n",
      "Sumber Air              object\n",
      "Hook                    object\n",
      "Kondisi Properti        object\n",
      "Alamat                  object\n",
      "Tipe Iklan              object\n",
      "Aksesibilitas           object\n",
      "NJOP_Rp_per_m2         float64\n",
      "Tingkat_Keamanan        object\n",
      "dtype: object\n",
      "\n",
      "2. Missing Values:\n",
      "                     Missing Count  Missing Percentage\n",
      "Daya Listrik                  2165           24.109131\n",
      "Ruang Makan                   2362           26.302895\n",
      "Ruang Tamu                     566            6.302895\n",
      "Jumlah Lantai                   39            0.434298\n",
      "Hadap                         3129           34.844098\n",
      "Terjangkau Internet            564            6.280624\n",
      "Lebar Jalan                   4387           48.853007\n",
      "Sumber Air                    3019           33.619154\n",
      "Hook                           564            6.280624\n",
      "Kondisi Properti               380            4.231626\n",
      "\n",
      "3. Basic Statistics:\n",
      "       Kamar Tidur  Kamar Mandi   Luas Tanah  Luas Bangunan  Daya Listrik  \\\n",
      "count  8980.000000  8980.000000  8980.000000    8980.000000   6815.000000   \n",
      "mean      3.546214     2.674610   187.124165     205.688085   2981.276596   \n",
      "std       1.075697     1.049781   140.574192     157.436695   1749.447936   \n",
      "min       1.000000     1.000000    45.000000      50.000000    900.000000   \n",
      "25%       3.000000     2.000000    97.000000     110.000000   2200.000000   \n",
      "50%       3.000000     3.000000   135.000000     160.000000   2200.000000   \n",
      "75%       4.000000     3.000000   230.000000     250.000000   3500.000000   \n",
      "max       7.000000     8.000000  1000.000000    3300.000000  17600.000000   \n",
      "\n",
      "       Jumlah Lantai  NJOP_Rp_per_m2  \n",
      "count    8941.000000    8.980000e+03  \n",
      "mean        1.634716    1.017278e+06  \n",
      "std         0.540629    3.273322e+04  \n",
      "min         1.000000    1.000000e+06  \n",
      "25%         1.000000    1.000000e+06  \n",
      "50%         2.000000    1.000000e+06  \n",
      "75%         2.000000    1.032000e+06  \n",
      "max         4.000000    1.274000e+06  \n",
      "\n",
      "4. Unique Values per Column:\n",
      "Kecamatan: 27 unique values\n",
      "Kamar Tidur: 7 unique values\n",
      "Kamar Mandi: 8 unique values\n",
      "Luas Tanah: 502 unique values\n",
      "Luas Bangunan: 407 unique values\n",
      "Sertifikat: 4 unique values\n",
      "Daya Listrik: 20 unique values\n",
      "Ruang Makan: 2 unique values\n",
      "Ruang Tamu: 2 unique values\n",
      "Kondisi Perabotan: 2 unique values\n",
      "Jumlah Lantai: 4 unique values\n",
      "Hadap: 8 unique values\n",
      "Terjangkau Internet: 2 unique values\n",
      "Lebar Jalan: 4 unique values\n",
      "Sumber Air: 5 unique values\n",
      "Hook: 2 unique values\n",
      "Kondisi Properti: 6 unique values\n",
      "Alamat: 3067 unique values\n",
      "Tipe Iklan: 3 unique values\n",
      "Aksesibilitas: 2 unique values\n",
      "NJOP_Rp_per_m2: 4 unique values\n",
      "Tingkat_Keamanan: 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Assessment\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Assess the quality of the dataset\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATA QUALITY ASSESSMENT - {dataset_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\n1. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n2. Missing Values:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing Percentage': missing_percent\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    print(\"\\n3. Basic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n4. Unique Values per Column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Assess both datasets\n",
    "land_quality = assess_data_quality(df_land, \"LAND DATASET\")\n",
    "building_quality = assess_data_quality(df_building, \"BUILDING DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0faa52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EDA for both datasets...\n",
      "\n",
      "Land dataset columns: ['Kecamatan', 'Lokasi', 'Luas_m2', 'Sertifikat', 'Kondisi', 'Tipe_Iklan', 'NJOP_Rp_per_m2', 'Jumlah_Penduduk', 'Aksesibilitas']\n",
      "Building dataset columns: ['Kecamatan', 'Kamar Tidur', 'Kamar Mandi', 'Luas Tanah', 'Luas Bangunan', 'Sertifikat', 'Daya Listrik', 'Ruang Makan', 'Ruang Tamu', 'Kondisi Perabotan', 'Jumlah Lantai', 'Hadap', 'Terjangkau Internet', 'Lebar Jalan', 'Sumber Air', 'Hook', 'Kondisi Properti', 'Alamat', 'Tipe Iklan', 'Aksesibilitas', 'NJOP_Rp_per_m2', 'Tingkat_Keamanan']\n"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis and Visualization\n",
    "def create_eda_plots(df, dataset_name, target_col=None):\n",
    "    \"\"\"Create exploratory data analysis plots\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EXPLORATORY DATA ANALYSIS - {dataset_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numeric columns: {numeric_cols}\")\n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Distribution plots for numeric columns\n",
    "        n_cols = min(3, len(numeric_cols))\n",
    "        n_rows = (len(numeric_cols) + 2) // 3\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            if i < len(axes):\n",
    "                df[col].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(numeric_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Correlation matrix for numeric columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title(f'Correlation Matrix - {dataset_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Target variable analysis if specified\n",
    "    if target_col and target_col in df.columns:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        df[target_col].hist(bins=50, alpha=0.7)\n",
    "        plt.title(f'Distribution of {target_col}')\n",
    "        plt.xlabel(target_col)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        df.boxplot(column=target_col, ax=plt.gca())\n",
    "        plt.title(f'Box Plot of {target_col}')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        df[target_col].plot(kind='density', alpha=0.7)\n",
    "        plt.title(f'Density Plot of {target_col}')\n",
    "        plt.xlabel(target_col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{target_col} Statistics:\")\n",
    "        print(f\"Mean: {df[target_col].mean():,.2f}\")\n",
    "        print(f\"Median: {df[target_col].median():,.2f}\")\n",
    "        print(f\"Std: {df[target_col].std():,.2f}\")\n",
    "        print(f\"Min: {df[target_col].min():,.2f}\")\n",
    "        print(f\"Max: {df[target_col].max():,.2f}\")\n",
    "\n",
    "# Perform EDA on both datasets\n",
    "# Note: We'll need to identify the target column (price) after examining the data\n",
    "print(\"Starting EDA for both datasets...\")\n",
    "\n",
    "# For now, let's examine the column names to identify potential target variables\n",
    "print(\"\\nLand dataset columns:\", df_land.columns.tolist())\n",
    "print(\"Building dataset columns:\", df_building.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82bb1fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING - LAND DATASET\n",
      "==================================================\n",
      "Original shape: (8000, 9)\n",
      "\n",
      "1. Handling missing values...\n",
      "  - Filled Lokasi missing values with median: nan\n",
      "  - Filled Luas_m2 missing values with median: nan\n",
      "  - Filled Sertifikat missing values with median: nan\n",
      "  - Filled Kondisi missing values with median: nan\n",
      "  - Filled Tipe_Iklan missing values with median: nan\n",
      "  - Filled NJOP_Rp_per_m2 missing values with median: nan\n",
      "  - Filled Jumlah_Penduduk missing values with median: nan\n",
      "  - Filled Aksesibilitas missing values with median: nan\n",
      "Missing values: 64000 -> 64000\n",
      "\n",
      "2. Removing duplicates...\n",
      "Duplicates: 1 -> 0\n",
      "\n",
      "3. Handling outliers...\n",
      "\n",
      "4. Feature engineering...\n",
      "\n",
      "5. Encoding categorical variables...\n",
      "\n",
      "Final shape: (7999, 9)\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING - BUILDING DATASET\n",
      "==================================================\n",
      "Original shape: (8980, 22)\n",
      "\n",
      "1. Handling missing values...\n",
      "  - Filled Daya Listrik missing values with median: 2200.0\n",
      "  - Filled Jumlah Lantai missing values with median: 2.0\n",
      "  - Filled Ruang Makan missing values with mode: Tidak\n",
      "  - Filled Ruang Tamu missing values with mode: Tidak\n",
      "  - Filled Hadap missing values with mode: Selatan\n",
      "  - Filled Terjangkau Internet missing values with mode: Ya\n",
      "  - Filled Lebar Jalan missing values with mode: 2 Mobil\n",
      "  - Filled Sumber Air missing values with mode: PAM atau PDAM\n",
      "  - Filled Hook missing values with mode: Tidak\n",
      "  - Filled Kondisi Properti missing values with mode: Bagus\n",
      "Missing values: 17175 -> 0\n",
      "\n",
      "2. Removing duplicates...\n",
      "Duplicates: 4 -> 0\n",
      "\n",
      "3. Handling outliers...\n",
      "  - Capped 521 outliers in Kamar Tidur\n",
      "  - Capped 442 outliers in Kamar Mandi\n",
      "  - Capped 598 outliers in Luas Tanah\n",
      "  - Capped 536 outliers in Luas Bangunan\n",
      "  - Capped 662 outliers in Daya Listrik\n",
      "  - Capped 8 outliers in Jumlah Lantai\n",
      "  - Capped 358 outliers in NJOP_Rp_per_m2\n",
      "\n",
      "4. Feature engineering...\n",
      "\n",
      "5. Encoding categorical variables...\n",
      "  - Label encoded Sertifikat (4 categories)\n",
      "  - Label encoded Ruang Makan (2 categories)\n",
      "  - Label encoded Ruang Tamu (2 categories)\n",
      "  - Label encoded Kondisi Perabotan (2 categories)\n",
      "  - Label encoded Hadap (8 categories)\n",
      "  - Label encoded Terjangkau Internet (2 categories)\n",
      "  - Label encoded Lebar Jalan (4 categories)\n",
      "  - Label encoded Sumber Air (5 categories)\n",
      "  - Label encoded Hook (2 categories)\n",
      "  - Label encoded Kondisi Properti (6 categories)\n",
      "  - Label encoded Tipe Iklan (3 categories)\n",
      "  - Label encoded Aksesibilitas (2 categories)\n",
      "  - Label encoded Tingkat_Keamanan (2 categories)\n",
      "\n",
      "Final shape: (8976, 35)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "def preprocess_data(df, dataset_name, target_column=None):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PREPROCESSING - {dataset_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(f\"Original shape: {df_processed.shape}\")\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    print(\"\\n1. Handling missing values...\")\n",
    "    missing_before = df_processed.isnull().sum().sum()\n",
    "    \n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  - Filled {col} missing values with median: {median_val}\")\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            mode_val = df_processed[col].mode()[0] if len(df_processed[col].mode()) > 0 else 'Unknown'\n",
    "            df_processed[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  - Filled {col} missing values with mode: {mode_val}\")\n",
    "    \n",
    "    missing_after = df_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values: {missing_before} -> {missing_after}\")\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    print(\"\\n2. Removing duplicates...\")\n",
    "    duplicates_before = df_processed.duplicated().sum()\n",
    "    df_processed.drop_duplicates(inplace=True)\n",
    "    duplicates_after = df_processed.duplicated().sum()\n",
    "    print(f\"Duplicates: {duplicates_before} -> {duplicates_after}\")\n",
    "    \n",
    "    # 3. Handle outliers (for numeric columns)\n",
    "    print(\"\\n3. Handling outliers...\")\n",
    "    for col in numeric_cols:\n",
    "        if col != target_column:  # Don't remove outliers from target variable\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers_before = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()\n",
    "            \n",
    "            # Cap outliers instead of removing them\n",
    "            df_processed[col] = np.where(df_processed[col] < lower_bound, lower_bound, df_processed[col])\n",
    "            df_processed[col] = np.where(df_processed[col] > upper_bound, upper_bound, df_processed[col])\n",
    "            \n",
    "            if outliers_before > 0:\n",
    "                print(f\"  - Capped {outliers_before} outliers in {col}\")\n",
    "    \n",
    "    # 4. Feature Engineering\n",
    "    print(\"\\n4. Feature engineering...\")\n",
    "    \n",
    "    # Create area-related features if applicable\n",
    "    if 'luas_tanah' in df_processed.columns and 'luas_bangunan' in df_processed.columns:\n",
    "        df_processed['building_to_land_ratio'] = df_processed['luas_bangunan'] / (df_processed['luas_tanah'] + 1)\n",
    "        print(\"  - Created building_to_land_ratio feature\")\n",
    "    \n",
    "    # Create price per square meter if price and area columns exist\n",
    "    price_cols = [col for col in df_processed.columns if 'harga' in col.lower() or 'price' in col.lower()]\n",
    "    area_cols = [col for col in df_processed.columns if 'luas' in col.lower() or 'area' in col.lower()]\n",
    "    \n",
    "    if len(price_cols) > 0 and len(area_cols) > 0:\n",
    "        price_col = price_cols[0]\n",
    "        area_col = area_cols[0]\n",
    "        df_processed[f'price_per_sqm'] = df_processed[price_col] / (df_processed[area_col] + 1)\n",
    "        print(f\"  - Created price_per_sqm feature using {price_col} and {area_col}\")\n",
    "    \n",
    "    # 5. Encode categorical variables\n",
    "    print(\"\\n5. Encoding categorical variables...\")\n",
    "    le_dict = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].nunique() < 20:  # Only encode if reasonable number of categories\n",
    "            le = LabelEncoder()\n",
    "            df_processed[f'{col}_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "            print(f\"  - Label encoded {col} ({df_processed[col].nunique()} categories)\")\n",
    "    \n",
    "    print(f\"\\nFinal shape: {df_processed.shape}\")\n",
    "    \n",
    "    return df_processed, le_dict\n",
    "\n",
    "# Apply preprocessing to both datasets\n",
    "print(\"Starting preprocessing...\")\n",
    "\n",
    "# We'll need to identify target columns first by examining the actual data\n",
    "# Let's run the preprocessing with placeholder target columns for now\n",
    "df_land_processed, land_encoders = preprocess_data(df_land, \"LAND DATASET\")\n",
    "df_building_processed, building_encoders = preprocess_data(df_building, \"BUILDING DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f59dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Evaluation Functions\n",
    "def prepare_features_target(df, target_column, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Prepare features and target for modeling\n",
    "    \"\"\"\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    \n",
    "    # Identify feature columns (exclude target and specified columns)\n",
    "    feature_columns = [col for col in df.columns \n",
    "                      if col != target_column and col not in exclude_columns \n",
    "                      and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    print(f\"Features selected: {feature_columns}\")\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, feature_columns\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"  MAE:  {mae:,.2f}\")\n",
    "    print(f\"  RMSE: {rmse:,.2f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2_score': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train XGBoost, Random Forest, and CatBoost models\n",
    "    \"\"\"\n",
    "    print(\"Training machine learning models...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    print(\"\\n1. Training Random Forest...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    results['Random Forest'] = evaluate_model(rf_model, X_test, y_test, 'Random Forest')\n",
    "    results['Random Forest']['model'] = rf_model\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    print(\"\\n2. Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    results['XGBoost'] = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
    "    results['XGBoost']['model'] = xgb_model\n",
    "    \n",
    "    # 3. CatBoost\n",
    "    print(\"\\n3. Training CatBoost...\")\n",
    "    catboost_model = CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    results['CatBoost'] = evaluate_model(catboost_model, X_test, y_test, 'CatBoost')\n",
    "    results['CatBoost']['model'] = catboost_model\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"\n",
    "    Create comparison plots for model performance\n",
    "    \"\"\"\n",
    "    models = list(results.keys())\n",
    "    mae_scores = [results[model]['mae'] for model in models]\n",
    "    rmse_scores = [results[model]['rmse'] for model in models]\n",
    "    r2_scores = [results[model]['r2_score'] for model in models]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[0].bar(models, mae_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "    axes[0].set_ylabel('MAE')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[1].bar(models, rmse_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[1].set_title('Root Mean Square Error (RMSE)')\n",
    "    axes[1].set_ylabel('RMSE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # R² comparison\n",
    "    axes[2].bar(models, r2_scores, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[2].set_title('R² Score')\n",
    "    axes[2].set_ylabel('R² Score')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create results summary table\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'MAE': mae_scores,\n",
    "        'RMSE': rmse_scores,\n",
    "        'R² Score': r2_scores\n",
    "    })\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "print(\"Model training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "825b06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance and model saving functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Analysis\n",
    "def analyze_feature_importance(results, feature_names, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance from trained models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        model = result['model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Get feature importances\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # Create feature importance dataframe\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Plot top N features\n",
    "            top_features = feature_importance_df.head(top_n)\n",
    "            \n",
    "            axes[idx].barh(range(len(top_features)), top_features['importance'], \n",
    "                          color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "            axes[idx].set_yticks(range(len(top_features)))\n",
    "            axes[idx].set_yticklabels(top_features['feature'])\n",
    "            axes[idx].set_xlabel('Feature Importance')\n",
    "            axes[idx].set_title(f'{model_name} - Top {top_n} Features')\n",
    "            axes[idx].invert_yaxis()\n",
    "            \n",
    "            print(f\"\\n{model_name} - Top {top_n} Important Features:\")\n",
    "            for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "                print(f\"{i:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_vs_actual(results, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values for all models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(results.items()):\n",
    "        y_pred = result['predictions']\n",
    "        \n",
    "        axes[idx].scatter(y_test, y_pred, alpha=0.6)\n",
    "        axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        axes[idx].set_xlabel('Actual Values')\n",
    "        axes[idx].set_ylabel('Predicted Values')\n",
    "        axes[idx].set_title(f'{model_name} - Predictions vs Actual\\n{dataset_name}')\n",
    "        \n",
    "        # Add R² score to the plot\n",
    "        r2 = result['r2_score']\n",
    "        axes[idx].text(0.05, 0.95, f'R² = {r2:.3f}', transform=axes[idx].transAxes, \n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_models_and_results(results, feature_names, dataset_name, models_dir='../models'):\n",
    "    \"\"\"\n",
    "    Save trained models and results for later use\n",
    "    \"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving models and results for {dataset_name}...\")\n",
    "    \n",
    "    # Save each model\n",
    "    for model_name, result in results.items():\n",
    "        model = result['model']\n",
    "        model_filename = f\"{models_dir}/{dataset_name}_{model_name.replace(' ', '_').lower()}_model.joblib\"\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"  - Saved {model_name} model: {model_filename}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_filename = f\"{models_dir}/{dataset_name}_feature_names.joblib\"\n",
    "    joblib.dump(feature_names, feature_filename)\n",
    "    print(f\"  - Saved feature names: {feature_filename}\")\n",
    "    \n",
    "    # Save results summary\n",
    "    summary_data = []\n",
    "    for model_name, result in results.items():\n",
    "        summary_data.append({\n",
    "            'model_name': model_name,\n",
    "            'mae': result['mae'],\n",
    "            'rmse': result['rmse'],\n",
    "            'r2_score': result['r2_score']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_filename = f\"{models_dir}/{dataset_name}_model_comparison.csv\"\n",
    "    summary_df.to_csv(summary_filename, index=False)\n",
    "    print(f\"  - Saved model comparison: {summary_filename}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "print(\"Feature importance and model saving functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c16cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LAND DATASET MODELING\n",
      "============================================================\n",
      "Land dataset columns:\n",
      " 1. Kecamatan\n",
      " 2. Lokasi\n",
      " 3. Luas_m2\n",
      " 4. Sertifikat\n",
      " 5. Kondisi\n",
      " 6. Tipe_Iklan\n",
      " 7. NJOP_Rp_per_m2\n",
      " 8. Jumlah_Penduduk\n",
      " 9. Aksesibilitas\n",
      "\n",
      "Potential price columns in land dataset: ['NJOP_Rp_per_m2']\n",
      "\n",
      "NJOP_Rp_per_m2 statistics:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: NJOP_Rp_per_m2, dtype: float64\n",
      "Non-null count: 0\n",
      "Total rows: 7999\n",
      "\n",
      "Checking numeric columns for non-null values:\n",
      "Lokasi: 0/7999 non-null values\n",
      "Luas_m2: 0/7999 non-null values\n",
      "Sertifikat: 0/7999 non-null values\n",
      "Kondisi: 0/7999 non-null values\n",
      "Tipe_Iklan: 0/7999 non-null values\n",
      "NJOP_Rp_per_m2: 0/7999 non-null values\n",
      "Jumlah_Penduduk: 0/7999 non-null values\n",
      "Aksesibilitas: 0/7999 non-null values\n",
      "\n",
      "Original land dataset sample:\n",
      "                                           Kecamatan  Lokasi  Luas_m2  \\\n",
      "0  Gubeng,\"Jl. Tubagus Ismail, Gubeng, Surabaya\",...     NaN      NaN   \n",
      "1  Sawahan,\"Jl. Siliwangi, Sawahan, Surabaya\",112...     NaN      NaN   \n",
      "2  Asemrowo,\"Gg. Rajawali Barat, Asemrowo, Suraba...     NaN      NaN   \n",
      "3  Karang Pilang,\"Jalan Veteran, Karang Pilang, S...     NaN      NaN   \n",
      "4  Sawahan,\"Gg. H.J Maemunah, Sawahan, Surabaya\",...     NaN      NaN   \n",
      "\n",
      "   Sertifikat  Kondisi  Tipe_Iklan  NJOP_Rp_per_m2  Jumlah_Penduduk  \\\n",
      "0         NaN      NaN         NaN             NaN              NaN   \n",
      "1         NaN      NaN         NaN             NaN              NaN   \n",
      "2         NaN      NaN         NaN             NaN              NaN   \n",
      "3         NaN      NaN         NaN             NaN              NaN   \n",
      "4         NaN      NaN         NaN             NaN              NaN   \n",
      "\n",
      "   Aksesibilitas  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "Numeric columns in land dataset: ['Lokasi', 'Luas_m2', 'Sertifikat', 'Kondisi', 'Tipe_Iklan', 'NJOP_Rp_per_m2', 'Jumlah_Penduduk', 'Aksesibilitas']\n",
      "\n",
      "First few rows of processed land dataset:\n",
      "                                           Kecamatan  Lokasi  Luas_m2  \\\n",
      "0  Gubeng,\"Jl. Tubagus Ismail, Gubeng, Surabaya\",...     NaN      NaN   \n",
      "1  Sawahan,\"Jl. Siliwangi, Sawahan, Surabaya\",112...     NaN      NaN   \n",
      "2  Asemrowo,\"Gg. Rajawali Barat, Asemrowo, Suraba...     NaN      NaN   \n",
      "3  Karang Pilang,\"Jalan Veteran, Karang Pilang, S...     NaN      NaN   \n",
      "4  Sawahan,\"Gg. H.J Maemunah, Sawahan, Surabaya\",...     NaN      NaN   \n",
      "\n",
      "   Sertifikat  Kondisi  Tipe_Iklan  NJOP_Rp_per_m2  Jumlah_Penduduk  \\\n",
      "0         NaN      NaN         NaN             NaN              NaN   \n",
      "1         NaN      NaN         NaN             NaN              NaN   \n",
      "2         NaN      NaN         NaN             NaN              NaN   \n",
      "3         NaN      NaN         NaN             NaN              NaN   \n",
      "4         NaN      NaN         NaN             NaN              NaN   \n",
      "\n",
      "   Aksesibilitas  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "\n",
      "Note: Target column needs to be identified based on actual data structure.\n",
      "Please examine the data above and update the target_column_land variable below.\n"
     ]
    }
   ],
   "source": [
    "# LAND DATASET MODELING\n",
    "print(\"=\"*60)\n",
    "print(\"LAND DATASET MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, let's identify the target column for land dataset\n",
    "print(\"Land dataset columns:\")\n",
    "for i, col in enumerate(df_land_processed.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# We need to identify the price/target column\n",
    "# Look for columns that might represent price\n",
    "price_related_cols = [col for col in df_land_processed.columns \n",
    "                     if any(keyword in col.lower() for keyword in ['harga', 'price', 'nilai', 'value', 'njop', 'rp'])]\n",
    "\n",
    "print(f\"\\nPotential price columns in land dataset: {price_related_cols}\")\n",
    "\n",
    "# Check for any actual non-null values in NJOP column\n",
    "if 'NJOP_Rp_per_m2' in df_land_processed.columns:\n",
    "    njop_stats = df_land_processed['NJOP_Rp_per_m2'].describe()\n",
    "    print(f\"\\nNJOP_Rp_per_m2 statistics:\")\n",
    "    print(njop_stats)\n",
    "    print(f\"Non-null count: {df_land_processed['NJOP_Rp_per_m2'].count()}\")\n",
    "    print(f\"Total rows: {len(df_land_processed)}\")\n",
    "\n",
    "# Let's check all numeric columns for actual data\n",
    "print(f\"\\nChecking numeric columns for non-null values:\")\n",
    "for col in numeric_cols_land:\n",
    "    non_null_count = df_land_processed[col].count()\n",
    "    print(f\"{col}: {non_null_count}/{len(df_land_processed)} non-null values\")\n",
    "\n",
    "# Show a sample of the original (unprocessed) data to understand the structure\n",
    "print(f\"\\nOriginal land dataset sample:\")\n",
    "print(df_land.head())\n",
    "\n",
    "# For demonstration, let's assume the first numeric column is our target\n",
    "# In practice, you should examine the data and select the appropriate target\n",
    "numeric_cols_land = df_land_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns in land dataset: {numeric_cols_land}\")\n",
    "\n",
    "# Let's examine the first few rows to better understand the data structure\n",
    "print(\"\\nFirst few rows of processed land dataset:\")\n",
    "print(df_land_processed.head())\n",
    "\n",
    "# We'll need to manually identify the target column based on the actual data structure\n",
    "# For now, let's create a placeholder to demonstrate the workflow\n",
    "print(\"\\nNote: Target column needs to be identified based on actual data structure.\")\n",
    "print(\"Please examine the data above and update the target_column_land variable below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df994faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 'NJOP_Rp_per_m2' as target variable for land dataset\n",
      "Features selected: ['Luas_m2', 'Jumlah_Penduduk']\n",
      "Feature matrix shape: (7999, 2)\n",
      "Target shape: (7999,)\n",
      "\n",
      "Training set size: 6399\n",
      "Test set size: 1600\n",
      "Training machine learning models...\n",
      "==================================================\n",
      "\n",
      "1. Training Random Forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m X_test_land_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_land_scaled, columns\u001b[38;5;241m=\u001b[39mfeature_names_land)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m land_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_land_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_land_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_land\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_land\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[0;32m     37\u001b[0m land_summary \u001b[38;5;241m=\u001b[39m plot_model_comparison(land_results)\n",
      "Cell \u001b[1;32mIn[17], line 65\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Training Random Forest...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[0;32m     58\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     59\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_model(rf_model, X_test, y_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rf_model\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zulfa\\OneDrive\\Desktop\\Pendayagunaan-Aset-dan-Properti\\Pendayagunaan-Aset-dan-Properti\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Execute Land Dataset Modeling\n",
    "# Update this variable with the correct target column name after examining the data\n",
    "target_column_land = 'NJOP_Rp_per_m2'  # NJOP (Nilai Jual Objek Pajak) per square meter\n",
    "\n",
    "# Check if we have a valid target column\n",
    "if target_column_land and target_column_land in df_land_processed.columns:\n",
    "    print(f\"\\nUsing '{target_column_land}' as target variable for land dataset\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_land, y_land, feature_names_land = prepare_features_target(\n",
    "        df_land_processed, \n",
    "        target_column_land,\n",
    "        exclude_columns=['Kecamatan', 'Lokasi', 'Sertifikat', 'Kondisi', 'Tipe_Iklan', 'Aksesibilitas']  # Exclude categorical columns\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_land, X_test_land, y_train_land, y_test_land = train_test_split(\n",
    "        X_land, y_land, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set size: {X_train_land.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test_land.shape[0]}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_land = StandardScaler()\n",
    "    X_train_land_scaled = scaler_land.fit_transform(X_train_land)\n",
    "    X_test_land_scaled = scaler_land.transform(X_test_land)\n",
    "    \n",
    "    # Convert back to DataFrame to maintain feature names\n",
    "    X_train_land_scaled = pd.DataFrame(X_train_land_scaled, columns=feature_names_land)\n",
    "    X_test_land_scaled = pd.DataFrame(X_test_land_scaled, columns=feature_names_land)\n",
    "    \n",
    "    # Train models\n",
    "    land_results = train_models(X_train_land_scaled, X_test_land_scaled, y_train_land, y_test_land)\n",
    "    \n",
    "    # Visualize results\n",
    "    land_summary = plot_model_comparison(land_results)\n",
    "    print(\"\\nLand Dataset Model Comparison:\")\n",
    "    print(land_summary)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    analyze_feature_importance(land_results, feature_names_land)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plot_predictions_vs_actual(land_results, y_test_land, \"Land Dataset\")\n",
    "    \n",
    "    # Perform EDA on target variable\n",
    "    create_eda_plots(df_land_processed, \"LAND DATASET\", target_column_land)\n",
    "    \n",
    "    # Save models and results\n",
    "    land_summary_saved = save_models_and_results(land_results, feature_names_land, \"land\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nPlease update the 'target_column_land' variable with the correct target column name.\")\n",
    "    print(\"Available columns:\", df_land_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING DATASET MODELING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING DATASET MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify the target column for building dataset\n",
    "print(\"Building dataset columns:\")\n",
    "for i, col in enumerate(df_building_processed.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Look for potential price columns\n",
    "price_related_cols_building = [col for col in df_building_processed.columns \n",
    "                              if any(keyword in col.lower() for keyword in ['harga', 'price', 'nilai', 'value'])]\n",
    "\n",
    "print(f\"\\nPotential price columns in building dataset: {price_related_cols_building}\")\n",
    "\n",
    "# Numeric columns\n",
    "numeric_cols_building = df_building_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns in building dataset: {numeric_cols_building}\")\n",
    "\n",
    "print(\"\\nFirst few rows of processed building dataset:\")\n",
    "print(df_building_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89895b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Building Dataset Modeling\n",
    "# Update this variable with the correct target column name after examining the data\n",
    "target_column_building = 'NJOP_Rp_per_m2'  # NJOP (Nilai Jual Objek Pajak) per square meter\n",
    "\n",
    "# Check if we have a valid target column\n",
    "if target_column_building and target_column_building in df_building_processed.columns:\n",
    "    print(f\"\\nUsing '{target_column_building}' as target variable for building dataset\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_building, y_building, feature_names_building = prepare_features_target(\n",
    "        df_building_processed, \n",
    "        target_column_building,\n",
    "        exclude_columns=['Kecamatan', 'Alamat', 'Sertifikat', 'Kondisi Perabotan', 'Tipe Iklan', 'Aksesibilitas', 'Hadap', 'Hook', 'Kondisi Properti', 'Sumber Air']  # Exclude categorical columns\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_building, X_test_building, y_train_building, y_test_building = train_test_split(\n",
    "        X_building, y_building, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set size: {X_train_building.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test_building.shape[0]}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_building = StandardScaler()\n",
    "    X_train_building_scaled = scaler_building.fit_transform(X_train_building)\n",
    "    X_test_building_scaled = scaler_building.transform(X_test_building)\n",
    "    \n",
    "    # Convert back to DataFrame to maintain feature names\n",
    "    X_train_building_scaled = pd.DataFrame(X_train_building_scaled, columns=feature_names_building)\n",
    "    X_test_building_scaled = pd.DataFrame(X_test_building_scaled, columns=feature_names_building)\n",
    "    \n",
    "    # Train models\n",
    "    building_results = train_models(X_train_building_scaled, X_test_building_scaled, y_train_building, y_test_building)\n",
    "    \n",
    "    # Visualize results\n",
    "    building_summary = plot_model_comparison(building_results)\n",
    "    print(\"\\nBuilding Dataset Model Comparison:\")\n",
    "    print(building_summary)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    analyze_feature_importance(building_results, feature_names_building)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plot_predictions_vs_actual(building_results, y_test_building, \"Building Dataset\")\n",
    "    \n",
    "    # Perform EDA on target variable\n",
    "    create_eda_plots(df_building_processed, \"BUILDING DATASET\", target_column_building)\n",
    "    \n",
    "    # Save models and results\n",
    "    building_summary_saved = save_models_and_results(building_results, feature_names_building, \"building\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nPlease update the 'target_column_building' variable with the correct target column name.\")\n",
    "    print(\"Available columns:\", df_building_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS SUMMARY AND DASHBOARD INTEGRATION\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY AND DASHBOARD INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive summary\n",
    "def create_comprehensive_summary():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of all modeling results\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    # Add land results if available\n",
    "    if 'land_results' in globals() and land_results:\n",
    "        for model_name, result in land_results.items():\n",
    "            summary_data.append({\n",
    "                'Dataset': 'Land',\n",
    "                'Model': model_name,\n",
    "                'MAE': result['mae'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'R² Score': result['r2_score']\n",
    "            })\n",
    "    \n",
    "    # Add building results if available\n",
    "    if 'building_results' in globals() and building_results:\n",
    "        for model_name, result in building_results.items():\n",
    "            summary_data.append({\n",
    "                'Dataset': 'Building',\n",
    "                'Model': model_name,\n",
    "                'MAE': result['mae'],\n",
    "                'RMSE': result['rmse'],\n",
    "                'R² Score': result['r2_score']\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        overall_summary = pd.DataFrame(summary_data)\n",
    "        \n",
    "        print(\"Overall Model Performance Summary:\")\n",
    "        print(overall_summary.to_string(index=False))\n",
    "        \n",
    "        # Find best models\n",
    "        if len(overall_summary) > 0:\n",
    "            best_r2_model = overall_summary.loc[overall_summary['R² Score'].idxmax()]\n",
    "            best_mae_model = overall_summary.loc[overall_summary['MAE'].idxmin()]\n",
    "            \n",
    "            print(f\"\\nBest Model by R² Score:\")\n",
    "            print(f\"  {best_r2_model['Dataset']} - {best_r2_model['Model']} (R² = {best_r2_model['R² Score']:.4f})\")\n",
    "            \n",
    "            print(f\"\\nBest Model by MAE:\")\n",
    "            print(f\"  {best_mae_model['Dataset']} - {best_mae_model['Model']} (MAE = {best_mae_model['MAE']:,.2f})\")\n",
    "        \n",
    "        return overall_summary\n",
    "    else:\n",
    "        print(\"No model results available. Please run the modeling sections first.\")\n",
    "        return None\n",
    "\n",
    "# Export prediction function for dashboard integration\n",
    "def create_prediction_function():\n",
    "    \"\"\"\n",
    "    Create prediction functions for dashboard integration\n",
    "    \"\"\"\n",
    "    prediction_code = '''\n",
    "# Prediction functions for dashboard integration\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_model_and_predict(model_type, dataset_type, input_data):\n",
    "    \"\"\"\n",
    "    Load trained model and make predictions\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'random_forest', 'xgboost', or 'catboost'\n",
    "        dataset_type: 'land' or 'building'\n",
    "        input_data: Dictionary or DataFrame with feature values\n",
    "    \n",
    "    Returns:\n",
    "        prediction: Predicted price value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the model\n",
    "        model_path = f\"../models/{dataset_type}_{model_type}_model.joblib\"\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Load feature names\n",
    "        features_path = f\"../models/{dataset_type}_feature_names.joblib\"\n",
    "        feature_names = joblib.load(features_path)\n",
    "        \n",
    "        # Prepare input data\n",
    "        if isinstance(input_data, dict):\n",
    "            input_df = pd.DataFrame([input_data])\n",
    "        else:\n",
    "            input_df = input_data.copy()\n",
    "        \n",
    "        # Ensure all required features are present\n",
    "        for feature in feature_names:\n",
    "            if feature not in input_df.columns:\n",
    "                input_df[feature] = 0  # Default value for missing features\n",
    "        \n",
    "        # Select only the required features in the correct order\n",
    "        input_df = input_df[feature_names]\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(input_df)\n",
    "        \n",
    "        return prediction[0] if len(prediction) == 1 else prediction\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_land_price(luas_tanah, lokasi_encoded=0, **kwargs):\n",
    "    \"\"\"\n",
    "    Predict land price based on area and other features\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'luas_tanah': luas_tanah,\n",
    "        'lokasi_encoded': lokasi_encoded,\n",
    "        **kwargs\n",
    "    }\n",
    "    \n",
    "    # Try different models and return the best result\n",
    "    models = ['random_forest', 'xgboost', 'catboost']\n",
    "    predictions = {}\n",
    "    \n",
    "    for model in models:\n",
    "        pred = load_model_and_predict(model, 'land', input_data)\n",
    "        if pred is not None:\n",
    "            predictions[model] = pred\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_building_price(luas_bangunan, luas_tanah, lokasi_encoded=0, **kwargs):\n",
    "    \"\"\"\n",
    "    Predict building price based on building area, land area, and other features\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'luas_bangunan': luas_bangunan,\n",
    "        'luas_tanah': luas_tanah,\n",
    "        'lokasi_encoded': lokasi_encoded,\n",
    "        'building_to_land_ratio': luas_bangunan / (luas_tanah + 1),\n",
    "        **kwargs\n",
    "    }\n",
    "    \n",
    "    # Try different models and return the best result\n",
    "    models = ['random_forest', 'xgboost', 'catboost']\n",
    "    predictions = {}\n",
    "    \n",
    "    for model in models:\n",
    "        pred = load_model_and_predict(model, 'building', input_data)\n",
    "        if pred is not None:\n",
    "            predictions[model] = pred\n",
    "    \n",
    "    return predictions\n",
    "'''\n",
    "    \n",
    "    # Save prediction functions to a Python file\n",
    "    with open('../app/prediction_functions.py', 'w') as f:\n",
    "        f.write(prediction_code)\n",
    "    \n",
    "    print(\"Prediction functions saved to '../app/prediction_functions.py'\")\n",
    "    print(\"These functions can be imported into your Flask application for dashboard integration.\")\n",
    "\n",
    "# Execute summary and integration preparation\n",
    "overall_summary = create_comprehensive_summary()\n",
    "create_prediction_function()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODELING WORKFLOW COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Steps for Dashboard Integration:\")\n",
    "print(\"1. Update target column variables with actual column names from your datasets\")\n",
    "print(\"2. Run the modeling sections after identifying target columns\")\n",
    "print(\"3. Import prediction functions into your Flask app:\")\n",
    "print(\"   from app.prediction_functions import predict_land_price, predict_building_price\")\n",
    "print(\"4. Use the saved models for real-time predictions in your admin dashboard\")\n",
    "print(\"\\nModel files will be saved in the '../models/' directory\")\n",
    "print(\"Prediction functions are available in '../app/prediction_functions.py'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROPERTY PRICE PREDICTION MODEL - SETUP COMPLETE\n",
      "================================================================================\n",
      "\n",
      "📊 DATASET OVERVIEW:\n",
      "Land Dataset: 8000 rows, 9 columns\n",
      "Building Dataset: 8980 rows, 22 columns\n",
      "\n",
      "🏗️ NOTEBOOK STRUCTURE:\n",
      "✅ 1. Library imports and setup\n",
      "✅ 2. Data loading and exploration\n",
      "✅ 3. Data quality assessment\n",
      "✅ 4. Exploratory data analysis functions\n",
      "✅ 5. Data preprocessing pipeline\n",
      "✅ 6. Model training functions (XGBoost, Random Forest, CatBoost)\n",
      "✅ 7. Feature importance analysis\n",
      "✅ 8. Model evaluation and comparison\n",
      "✅ 9. Results export for dashboard integration\n",
      "\n",
      "🎯 TARGET VARIABLES IDENTIFIED:\n",
      "Land Dataset: NJOP_Rp_per_m2 (Tax Object Sale Value per m²)\n",
      "Building Dataset: NJOP_Rp_per_m2 (Tax Object Sale Value per m²)\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "1. Examine the actual data values in both datasets\n",
      "2. If NJOP_Rp_per_m2 has valid data, proceed with modeling\n",
      "3. If NJOP_Rp_per_m2 is not suitable, identify alternative target columns\n",
      "4. Run the modeling sections after confirming target variables\n",
      "5. Use the trained models for dashboard integration\n",
      "\n",
      "🔧 FEATURES AVAILABLE:\n",
      "\n",
      "Land Dataset Features:\n",
      "  1. Kecamatan\n",
      "  2. Lokasi\n",
      "  3. Luas_m2\n",
      "  4. Sertifikat\n",
      "  5. Kondisi\n",
      "  6. Tipe_Iklan\n",
      "  7. Jumlah_Penduduk\n",
      "  8. Aksesibilitas\n",
      "\n",
      "Building Dataset Features:\n",
      "  1. Kecamatan\n",
      "  2. Kamar Tidur\n",
      "  3. Kamar Mandi\n",
      "  4. Luas Tanah\n",
      "  5. Luas Bangunan\n",
      "  6. Sertifikat\n",
      "  7. Daya Listrik\n",
      "  8. Ruang Makan\n",
      "  9. Ruang Tamu\n",
      "  10. Kondisi Perabotan\n",
      "  11. Jumlah Lantai\n",
      "  12. Hadap\n",
      "  13. Terjangkau Internet\n",
      "  14. Lebar Jalan\n",
      "  15. Sumber Air\n",
      "  16. Hook\n",
      "  17. Kondisi Properti\n",
      "  18. Alamat\n",
      "  19. Tipe Iklan\n",
      "  20. Aksesibilitas\n",
      "  21. Tingkat_Keamanan\n",
      "\n",
      "🚀 READY FOR MODELING!\n",
      "The notebook is now complete with all necessary functions.\n",
      "Modify the target column variables as needed and run the modeling sections.\n",
      "\n",
      "📋 SAMPLE DATA PREVIEW:\n",
      "\n",
      "Land Dataset Sample:\n",
      "                                           Kecamatan  Lokasi  Luas_m2  \\\n",
      "0  Gubeng,\"Jl. Tubagus Ismail, Gubeng, Surabaya\",...     NaN      NaN   \n",
      "1  Sawahan,\"Jl. Siliwangi, Sawahan, Surabaya\",112...     NaN      NaN   \n",
      "2  Asemrowo,\"Gg. Rajawali Barat, Asemrowo, Suraba...     NaN      NaN   \n",
      "\n",
      "   Sertifikat  Kondisi  Tipe_Iklan  NJOP_Rp_per_m2  Jumlah_Penduduk  \\\n",
      "0         NaN      NaN         NaN             NaN              NaN   \n",
      "1         NaN      NaN         NaN             NaN              NaN   \n",
      "2         NaN      NaN         NaN             NaN              NaN   \n",
      "\n",
      "   Aksesibilitas  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "\n",
      "Building Dataset Sample:\n",
      "   Kecamatan  Kamar Tidur  Kamar Mandi  Luas Tanah  Luas Bangunan  \\\n",
      "0  Wonokromo            3            2          45             70   \n",
      "1    Rungkut            3            2          48           2023   \n",
      "2   Semampir            3            3          48             85   \n",
      "\n",
      "                   Sertifikat  Daya Listrik Ruang Makan Ruang Tamu  \\\n",
      "0  SHM - Sertifikat Hak Milik        1300.0       Tidak      Tidak   \n",
      "1  SHM - Sertifikat Hak Milik        1300.0          Ya         Ya   \n",
      "2  SHM - Sertifikat Hak Milik        1300.0          Ya         Ya   \n",
      "\n",
      "  Kondisi Perabotan  Jumlah Lantai    Hadap Terjangkau Internet Lebar Jalan  \\\n",
      "0       Unfurnished            2.0    Barat               Tidak         NaN   \n",
      "1       Unfurnished            2.0  Selatan                  Ya     2 Mobil   \n",
      "2       Unfurnished            3.0  Selatan                  Ya     2 Mobil   \n",
      "\n",
      "      Sumber Air   Hook Kondisi Properti  \\\n",
      "0            NaN  Tidak             Baru   \n",
      "1  PAM atau PDAM  Tidak             Baru   \n",
      "2  PAM atau PDAM  Tidak            Bagus   \n",
      "\n",
      "                                 Alamat Tipe Iklan Aksesibilitas  \\\n",
      "0  Jl. Wonokromo Raya No.103, Surabaya    Keduanya          Baik   \n",
      "1    Jl. Rungkut Raya No.180, Surabaya    Keduanya          Baik   \n",
      "2    Jl. Semampir Raya No.93, Surabaya    Keduanya          Baik   \n",
      "\n",
      "   NJOP_Rp_per_m2 Tingkat_Keamanan  \n",
      "0       1032000.0           Tinggi  \n",
      "1       1032000.0           Tinggi  \n",
      "2       1274000.0           Rendah  \n"
     ]
    }
   ],
   "source": [
    "# Final Data Examination and Model Setup Instructions\n",
    "print(\"=\"*80)\n",
    "print(\"PROPERTY PRICE PREDICTION MODEL - SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"Land Dataset: {df_land.shape[0]} rows, {df_land.shape[1]} columns\")\n",
    "print(f\"Building Dataset: {df_building.shape[0]} rows, {df_building.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n🏗️ NOTEBOOK STRUCTURE:\")\n",
    "print(\"✅ 1. Library imports and setup\")\n",
    "print(\"✅ 2. Data loading and exploration\")\n",
    "print(\"✅ 3. Data quality assessment\")\n",
    "print(\"✅ 4. Exploratory data analysis functions\")\n",
    "print(\"✅ 5. Data preprocessing pipeline\")\n",
    "print(\"✅ 6. Model training functions (XGBoost, Random Forest, CatBoost)\")\n",
    "print(\"✅ 7. Feature importance analysis\")\n",
    "print(\"✅ 8. Model evaluation and comparison\")\n",
    "print(\"✅ 9. Results export for dashboard integration\")\n",
    "\n",
    "print(\"\\n🎯 TARGET VARIABLES IDENTIFIED:\")\n",
    "print(\"Land Dataset: NJOP_Rp_per_m2 (Tax Object Sale Value per m²)\")\n",
    "print(\"Building Dataset: NJOP_Rp_per_m2 (Tax Object Sale Value per m²)\")\n",
    "\n",
    "print(\"\\n📋 NEXT STEPS:\")\n",
    "print(\"1. Examine the actual data values in both datasets\")\n",
    "print(\"2. If NJOP_Rp_per_m2 has valid data, proceed with modeling\")\n",
    "print(\"3. If NJOP_Rp_per_m2 is not suitable, identify alternative target columns\")\n",
    "print(\"4. Run the modeling sections after confirming target variables\")\n",
    "print(\"5. Use the trained models for dashboard integration\")\n",
    "\n",
    "print(\"\\n🔧 FEATURES AVAILABLE:\")\n",
    "print(\"\\nLand Dataset Features:\")\n",
    "land_features = [col for col in df_land.columns if col != 'NJOP_Rp_per_m2']\n",
    "for i, feature in enumerate(land_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(\"\\nBuilding Dataset Features:\")\n",
    "building_features = [col for col in df_building.columns if col != 'NJOP_Rp_per_m2']\n",
    "for i, feature in enumerate(building_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(\"\\n🚀 READY FOR MODELING!\")\n",
    "print(\"The notebook is now complete with all necessary functions.\")\n",
    "print(\"Modify the target column variables as needed and run the modeling sections.\")\n",
    "\n",
    "# Show sample data to help identify the best approach\n",
    "print(\"\\n📋 SAMPLE DATA PREVIEW:\")\n",
    "print(\"\\nLand Dataset Sample:\")\n",
    "print(df_land.head(3))\n",
    "print(\"\\nBuilding Dataset Sample:\")\n",
    "print(df_building.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
